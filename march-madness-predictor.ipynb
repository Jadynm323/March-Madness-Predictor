{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d925a431",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-23T00:05:11.489589Z",
     "iopub.status.busy": "2025-04-23T00:05:11.489280Z",
     "iopub.status.idle": "2025-04-23T00:05:34.382315Z",
     "shell.execute_reply": "2025-04-23T00:05:34.381333Z"
    },
    "papermill": {
     "duration": 22.899083,
     "end_time": "2025-04-23T00:05:34.383942",
     "exception": false,
     "start_time": "2025-04-23T00:05:11.484859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 00:05:18.575566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745366718.888642      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745366718.971944      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better notebook rendering\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# 1. Data Loading and Exploration\n",
    "def load_and_explore_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset and perform initial exploration.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of seasons: {df['Season'].nunique()}\")\n",
    "    print(f\"Seasons included: {sorted(df['Season'].unique())}\")\n",
    "    \n",
    "    # Display info about target variables\n",
    "    target_columns = ['Tournament Winner?', 'Tournament Championship?', 'Final Four?']\n",
    "    for col in target_columns:\n",
    "        if col in df.columns:\n",
    "            true_count = df[col].sum()\n",
    "            total_count = df[col].count()\n",
    "            print(f\"{col}: {true_count} out of {total_count} ({true_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    # Quick look at data\n",
    "    display(df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    display(missing_values[missing_values > 0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Feature Engineering and Selection\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer and select features for the model.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Create copy to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handling missing values\n",
    "    numeric_columns = df_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_processed[numeric_columns] = df_processed[numeric_columns].fillna(df_processed[numeric_columns].mean())\n",
    "    \n",
    "    # Feature engineering - Create some ratio features\n",
    "    # Offensive to Defensive efficiency ratio\n",
    "    if 'Adjusted Offensive Efficiency' in df_processed.columns and 'Adjusted Defensive Efficiency' in df_processed.columns:\n",
    "        df_processed['Off_Def_Efficiency_Ratio'] = df_processed['Adjusted Offensive Efficiency'] / df_processed['Adjusted Defensive Efficiency']\n",
    "    \n",
    "    # Experience to average height ratio (team experience level per height)\n",
    "    if 'Experience' in df_processed.columns and 'AvgHeight' in df_processed.columns:\n",
    "        df_processed['Experience_Height_Ratio'] = df_processed['Experience'] / df_processed['AvgHeight']\n",
    "    \n",
    "    # Net Tempo (how much faster team plays compared to opponents)\n",
    "    if 'Avg Possession Length (Offense)' in df_processed.columns and 'Avg Possession Length (Defense)' in df_processed.columns:\n",
    "        df_processed['Net_Possession_Length'] = df_processed['Avg Possession Length (Defense)'] - df_processed['Avg Possession Length (Offense)']\n",
    "    \n",
    "    # Tournament experience feature\n",
    "    if 'Season' in df_processed.columns and 'Full Team Name' in df_processed.columns:\n",
    "        # For each team, check how many previous seasons they appear in\n",
    "        team_appearances = df_processed.groupby(['Full Team Name', 'Season']).size().reset_index(name='count')\n",
    "        team_exp = team_appearances.groupby('Full Team Name').cumcount()\n",
    "        team_appearances['Tournament_Experience'] = team_exp\n",
    "        # Merge back to the main dataframe\n",
    "        df_processed = df_processed.merge(team_appearances[['Full Team Name', 'Season', 'Tournament_Experience']], \n",
    "                                         on=['Full Team Name', 'Season'], how='left')\n",
    "        df_processed['Tournament_Experience'] = df_processed['Tournament_Experience'].fillna(0)\n",
    "    \n",
    "    # Convert categorical features with one-hot encoding\n",
    "    categorical_cols = ['Short Conference Name', 'Region', 'Post-Season Tournament']\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols)\n",
    "    \n",
    "    # Feature selection - excluding target variables and non-predictive columns\n",
    "    exclude_columns = ['Tournament Winner?', 'Tournament Championship?', 'Final Four?', \n",
    "                      'Full Team Name', 'Mapped ESPN Team Name', 'Since', 'Current Coach']\n",
    "    \n",
    "    features = [col for col in df_processed.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Show feature count\n",
    "    print(f\"Total features after engineering: {len(features)}\")\n",
    "    \n",
    "    return df_processed, features\n",
    "\n",
    "# 3. Define target and prepare train/test sets\n",
    "def prepare_train_test_data(df, features, target_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare training and testing datasets for a specific target.\n",
    "    \"\"\"\n",
    "    print(f\"Preparing training and testing data for {target_column}...\")\n",
    "    \n",
    "    # Make sure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset\")\n",
    "    \n",
    "    # Handle missing values in target\n",
    "    if df[target_column].isna().any():\n",
    "        print(f\"Warning: {df[target_column].isna().sum()} missing values in target. Filling with False.\")\n",
    "        df[target_column] = df[target_column].fillna(False)\n",
    "    \n",
    "    # Convert boolean to int\n",
    "    if df[target_column].dtype == bool:\n",
    "        df[target_column] = df[target_column].astype(int)\n",
    "    \n",
    "    # Select only complete rows\n",
    "    valid_rows = df[features + [target_column]].dropna()\n",
    "    \n",
    "    # Handle potential class imbalance\n",
    "    positive_samples = valid_rows[valid_rows[target_column] == 1]\n",
    "    negative_samples = valid_rows[valid_rows[target_column] == 0]\n",
    "    \n",
    "    print(f\"Positive samples: {len(positive_samples)}, Negative samples: {len(negative_samples)}\")\n",
    "    \n",
    "    # If severe imbalance, consider balancing techniques\n",
    "    if len(positive_samples) / len(valid_rows) < 0.1:\n",
    "        print(\"Warning: Severe class imbalance detected\")\n",
    "        # Option: Undersample majority class if enough data\n",
    "        if len(negative_samples) > 10 * len(positive_samples):\n",
    "            neg_sample = negative_samples.sample(n=min(len(positive_samples) * 3, len(negative_samples)), \n",
    "                                                random_state=random_state)\n",
    "            balanced_data = pd.concat([positive_samples, neg_sample])\n",
    "            X = balanced_data[features]\n",
    "            y = balanced_data[target_column]\n",
    "            print(f\"Balanced dataset: {len(balanced_data)} samples\")\n",
    "        else:\n",
    "            X = valid_rows[features]\n",
    "            y = valid_rows[target_column]\n",
    "    else:\n",
    "        X = valid_rows[features]\n",
    "        y = valid_rows[target_column]\n",
    "    \n",
    "    # Identify potentially important features through correlation\n",
    "    top_corr = pd.DataFrame({'feature': features, \n",
    "                            'correlation': [abs(X[f].corr(y)) for f in features]})\n",
    "    top_features = top_corr.sort_values('correlation', ascending=False).head(20)\n",
    "    \n",
    "    print(\"\\nTop correlated features:\")\n",
    "    display(top_features)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, features\n",
    "\n",
    "# 4. Build and train the model\n",
    "def build_and_train_model(X_train, y_train, X_test, y_test, input_dim, class_weight=None):\n",
    "    \"\"\"\n",
    "    Build and train a TensorFlow model for the given dataset.\n",
    "    \"\"\"\n",
    "    print(\"Building and training the model...\")\n",
    "    \n",
    "    # Set up callbacks for early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Build the model\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test loss: {test_results[0]:.4f}\")\n",
    "    print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 5. Visualize training history\n",
    "def visualize_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualize the training and validation metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    axes[0].plot(history.history['accuracy'])\n",
    "    axes[0].plot(history.history['val_accuracy'])\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].legend(['Train', 'Validation'], loc='best')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    axes[1].plot(history.history['loss'])\n",
    "    axes[1].plot(history.history['val_loss'])\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].legend(['Train', 'Validation'], loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot AUC and other metrics if available\n",
    "    if 'auc' in history.history:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].plot(history.history['auc'])\n",
    "        axes[0].plot(history.history['val_auc'])\n",
    "        axes[0].set_title('Model AUC')\n",
    "        axes[0].set_ylabel('AUC')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].legend(['Train', 'Validation'], loc='best')\n",
    "        \n",
    "        # Plot precision and recall\n",
    "        if 'precision' in history.history:\n",
    "            axes[1].plot(history.history['precision'])\n",
    "            axes[1].plot(history.history['recall'])\n",
    "            axes[1].plot(history.history['val_precision'])\n",
    "            axes[1].plot(history.history['val_recall'])\n",
    "            axes[1].set_title('Precision and Recall')\n",
    "            axes[1].set_ylabel('Score')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].legend(['Precision', 'Recall', 'Val Precision', 'Val Recall'], loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 6. Make predictions and evaluate\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance with detailed metrics and visualizations.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "# 7. Create a prediction function\n",
    "def predict_team_performance(model, scaler, features, team_data):\n",
    "    \"\"\"\n",
    "    Predict performance for new teams.\n",
    "    \"\"\"\n",
    "    # Preprocess the team data\n",
    "    team_features = team_data[features]\n",
    "    team_features_scaled = scaler.transform(team_features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_prob = model.predict(team_features_scaled)\n",
    "    \n",
    "    return prediction_prob\n",
    "\n",
    "# 8. Feature importance analysis\n",
    "def analyze_feature_importance(model, features):\n",
    "    \"\"\"\n",
    "    Analyze which features are most important for predictions.\n",
    "    \"\"\"\n",
    "    # For a simple dense network, we can look at the weights of the first layer\n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    importance = np.abs(weights).mean(axis=1)\n",
    "    \n",
    "    # Create a DataFrame to display feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "    plt.title('Top 20 Most Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1dd91",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-22T23:55:13.336553Z",
     "iopub.status.busy": "2025-04-22T23:55:13.336115Z",
     "iopub.status.idle": "2025-04-22T23:55:13.678299Z",
     "shell.execute_reply": "2025-04-22T23:55:13.677355Z",
     "shell.execute_reply.started": "2025-04-22T23:55:13.336528Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.001476,
     "end_time": "2025-04-23T00:05:34.387801",
     "exception": false,
     "start_time": "2025-04-23T00:05:34.386325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4594179,
     "sourceId": 11519526,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.311831,
   "end_time": "2025-04-23T00:05:37.501391",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-23T00:05:06.189560",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
